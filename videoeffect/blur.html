<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Virtual Background Effect</title>
  <style>
    video,
    canvas {
      position: absolute;
      top: 20;
      left: 0;
      transform: scaleX(-1);
    }

    canvas {
      z-index: 1;
    }
  </style>
</head>

<body>
  <div id="status" style="font: 1em sans-serif"></div>
  <div style="margin-bottom: 10px;">
    <label style="font: 1em sans-serif;">
      <input type="checkbox" id="useGPUSegmentation" disabled/> Use GPU for segmentation (uncheck for CPU)
    </label>
  </div>
  <div style="margin-bottom: 10px;">
    <label style="font: 1em sans-serif;">
      <input type="checkbox" id="useWebGPU" /> Use WebGPU for blur rendering (uncheck for WebGL2)
    </label>
  </div>
  <div>
    <video id="webcam" autoplay playsinline muted></video>
    <canvas id="output"></canvas>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-segmentation"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/selfie_segmentation.js"
    crossorigin="anonymous"></script>
  <script>
    var QueryString = function() {
      // Allows access to query parameters on the URL; e.g., given a URL like:
      //    http://<server>/my.html?test=123&bob=123
      // Parameters can then be accessed via QueryString.test or QueryString.bob.
      var params = {};
      // RegEx to split out values by &.
      var r = /([^&=]+)=?([^&]*)/g;
      // Lambda function for decoding extracted match values. Replaces '+' with
      // space so decodeURIComponent functions properly.
      function d(s) { return decodeURIComponent(s.replace(/\+/g, ' ')); }
      var match;
      while (match = r.exec(window.location.search.substring(1)))
        params[d(match[1])] = d(match[2]);
      return params;
    }();

    // Get median value from an array of Number
    function getMedianValue(array) {
      array = array.sort((a, b) => a - b);
      return array.length % 2 !== 0 ? array[Math.floor(array.length / 2)] :
        (array[array.length / 2 - 1] + array[array.length / 2]) / 2;
    }

    async function run() {
      const startRun = performance.now();
      let count = 0;
      const segmentTimes = [];
      const status = document.getElementById('status');
      const video = document.getElementById('webcam');
      const canvas = document.getElementById('output');
      const ctx = canvas.getContext('2d');
      const useWebGPUCheckbox = document.getElementById('useWebGPU');
      const useGPUSegmentationCheckbox = document.getElementById('useGPUSegmentation');
      const vbFps = 24;
      let blurRenderer = null;
      let segmenter = null;

      // Initialize segmenter based on checkbox
      async function initializeSegmenter() {
        const useGPUSegmentation = useGPUSegmentationCheckbox.checked;
        try {
          if (useGPUSegmentation) {
            // Initialize TensorFlow.js backend before creating GPU segmenter
            console.log('Initializing TensorFlow.js backend...');
            await tf.ready();
            
            // Try WebGPU first if available, fallback to WebGL
            let backend = 'webgl';
            if ('gpu' in navigator && useWebGPUCheckbox.checked) {
              try {
                await tf.setBackend('webgpu');
                backend = 'webgpu';
                console.log('Using WebGPU backend for TensorFlow.js');
              } catch (error) {
                console.warn('WebGPU backend failed, falling back to WebGL:', error);
                await tf.setBackend('webgl');
                backend = 'webgl';
              }
            } else {
              await tf.setBackend('webgl');
            }
            
            console.log('TensorFlow.js backend initialized:', tf.getBackend());
            
            // GPU-based segmentation using TensorFlow.js with specific backend
            segmenter = await bodySegmentation.createSegmenter(
              bodySegmentation.SupportedModels.MediaPipeSelfieSegmentation,
              {
                runtime: 'tfjs',
                modelType: 'landscape',
                // Explicitly specify the backend for segmentation
                backend: backend
              }
            );
            console.log(`Using GPU (TensorFlow.js ${backend}) for segmentation`);
          } else {
            // CPU-based segmentation using MediaPipe
            segmenter = await bodySegmentation.createSegmenter(
              bodySegmentation.SupportedModels.MediaPipeSelfieSegmentation,
              {
                runtime: 'mediapipe',
                modelType: 'landscape',
                solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation',
              }
            );
            console.log('Using CPU (MediaPipe) for segmentation');
          }
          
          // Update status to show segmentation method
          const segmentationType = useGPUSegmentation ? 'GPU (TensorFlow.js)' : 'CPU (MediaPipe)';
          const rendererType = useWebGPUCheckbox.checked ? 'WebGPU' : 'WebGL2';
          status.innerText = `Segmentation: ${segmentationType} | Renderer: ${rendererType}`;
          
        } catch (error) {
          console.warn(`Failed to initialize ${useGPUSegmentation ? 'GPU' : 'CPU'} segmentation:`, error);
          // Fallback to CPU if GPU fails
          if (useGPUSegmentation) {
            segmenter = await bodySegmentation.createSegmenter(
              bodySegmentation.SupportedModels.MediaPipeSelfieSegmentation,
              {
                runtime: 'mediapipe',
                modelType: 'landscape',
                solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation',
              }
            );
            useGPUSegmentationCheckbox.checked = false;
            const rendererType = useWebGPUCheckbox.checked ? 'WebGPU' : 'WebGL2';
            status.innerText = `Segmentation: CPU (MediaPipe) (GPU fallback) | Renderer: ${rendererType}`;
          }
        }
      }

      // Initialize blur renderer based on checkbox
      async function initializeBlurRenderer() {
        const useWebGPU = useWebGPUCheckbox.checked;
        try {
          if (useWebGPU && 'gpu' in navigator) {
            blurRenderer = await createWebGPUBlurRenderer(canvas);
            status.innerText = 'Renderer: WebGPU';
            console.log('Using WebGPU for blur rendering');
          } else {
            blurRenderer = createWebGL2BlurRenderer(canvas);
            status.innerText = 'Renderer: WebGL2';
            console.log('Using WebGL2 for blur rendering');
          }
        } catch (error) {
          console.warn(`Failed to initialize ${useWebGPU ? 'WebGPU' : 'WebGL2'} renderer:`, error);
          // Fallback to WebGL2 if WebGPU fails
          if (useWebGPU) {
            blurRenderer = createWebGL2BlurRenderer(canvas);
            status.innerText = 'Renderer: WebGL2 (WebGPU fallback)';
            useWebGPUCheckbox.checked = false;
          }
        }
      }

      // WebGL2 blur renderer
      function createWebGL2BlurRenderer(canvas) {
        // Create a separate canvas for WebGL2 to avoid context conflicts
        const webglCanvas = document.createElement('canvas');
        webglCanvas.width = canvas.width;
        webglCanvas.height = canvas.height;
        
        const gl = webglCanvas.getContext('webgl2');
        if (!gl) throw new Error('WebGL2 not supported');

        // Simple blur implementation using WebGL2
        const vertexShaderSource = `#version 300 es
          in vec2 a_position;
          in vec2 a_texCoord;
          out vec2 v_texCoord;
          void main() {
            gl_Position = vec4(a_position, 0.0, 1.0);
            v_texCoord = a_texCoord;
          }
        `;

        const fragmentShaderSource = `#version 300 es
          precision highp float;
          in vec2 v_texCoord;
          out vec4 fragColor;
          uniform sampler2D u_image;
          uniform sampler2D u_mask;
          uniform vec2 u_resolution;
          uniform float u_blurAmount;
          
          vec4 blur(sampler2D image, vec2 uv, float amount) {
            vec4 color = vec4(0.0);
            float total = 0.0;
            for(int x = -4; x <= 4; x++) {
              for(int y = -4; y <= 4; y++) {
                vec2 offset = vec2(float(x), float(y)) * amount / u_resolution;
                float weight = 1.0 / (1.0 + length(vec2(x, y)));
                color += texture(image, uv + offset) * weight;
                total += weight;
              }
            }
            return color / total;
          }
          
          void main() {
            vec4 originalColor = texture(u_image, v_texCoord);
            vec4 blurredColor = blur(u_image, v_texCoord, u_blurAmount);
            float mask = texture(u_mask, v_texCoord).r;
            fragColor = mix(blurredColor, originalColor, mask);
          }
        `;

        // Create and compile shaders, program, etc.
        const program = createProgram(gl, vertexShaderSource, fragmentShaderSource);
        
        // Set up geometry and buffers
        const positions = new Float32Array([
          -1, -1, 0, 1,  // Bottom-left: flipped Y texture coordinate
           1, -1, 1, 1,  // Bottom-right: flipped Y texture coordinate
          -1,  1, 0, 0,  // Top-left: flipped Y texture coordinate
           1,  1, 1, 0,  // Top-right: flipped Y texture coordinate
        ]);
        
        const positionBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        gl.bufferData(gl.ARRAY_BUFFER, positions, gl.STATIC_DRAW);
        
        // Create textures
        const videoTexture = gl.createTexture();
        const maskTexture = gl.createTexture();
        
        const positionLocation = gl.getAttribLocation(program, 'a_position');
        const texCoordLocation = gl.getAttribLocation(program, 'a_texCoord');
        const imageLocation = gl.getUniformLocation(program, 'u_image');
        const maskLocation = gl.getUniformLocation(program, 'u_mask');
        const resolutionLocation = gl.getUniformLocation(program, 'u_resolution');
        const blurAmountLocation = gl.getUniformLocation(program, 'u_blurAmount');
        
        return {
          render: (videoElement, maskImageData) => {
            // WebGL2 rendering implementation with actual blur effect
            renderWithWebGL2(gl, program, videoElement, maskImageData, {
              positionBuffer, videoTexture, maskTexture,
              positionLocation, texCoordLocation, imageLocation, 
              maskLocation, resolutionLocation, blurAmountLocation,
              webglCanvas
            });
            
            // Copy the WebGL result to the main canvas
            const mainCtx = canvas.getContext('2d');
            mainCtx.clearRect(0, 0, canvas.width, canvas.height);
            mainCtx.drawImage(webglCanvas, 0, 0);
          }
        };
      }

      // WebGPU blur renderer
      async function createWebGPUBlurRenderer(canvas) {
        // Create a separate canvas for WebGPU to avoid context conflicts
        const webgpuCanvas = document.createElement('canvas');
        webgpuCanvas.width = canvas.width;
        webgpuCanvas.height = canvas.height;
        
        const adapter = await navigator.gpu.requestAdapter();
        if (!adapter) {
          throw new Error('WebGPU adapter not available');
        }
        
        const device = await adapter.requestDevice();
        const context = webgpuCanvas.getContext('webgpu');
        
        if (!context) {
          throw new Error('WebGPU context not available');
        }
        
        const format = 'bgra8unorm';
        context.configure({
          device: device,
          format: format,
        });

        // WebGPU compute shader for blur effect
        const computeShaderCode = `
          @group(0) @binding(0) var inputTexture: texture_2d<f32>;
          @group(0) @binding(1) var maskTexture: texture_2d<f32>;
          @group(0) @binding(2) var outputTexture: texture_storage_2d<rgba8unorm, write>;
          
          @compute @workgroup_size(8, 8)
          fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
            let inputDims = textureDimensions(inputTexture);
            let maskDims = textureDimensions(maskTexture);
            
            if (global_id.x >= inputDims.x || global_id.y >= inputDims.y) {
              return;
            }
            
            let coord = vec2<i32>(i32(global_id.x), i32(global_id.y));
            
            // Sample the original color
            let originalColor = textureLoad(inputTexture, coord, 0);
            
            // Calculate corresponding mask coordinate (handle different dimensions)
            let maskCoord = vec2<i32>(
              i32(f32(global_id.x) * f32(maskDims.x) / f32(inputDims.x)),
              i32(f32(global_id.y) * f32(maskDims.y) / f32(inputDims.y))
            );
            
            // Clamp mask coordinates to valid range
            let clampedMaskCoord = vec2<i32>(
              clamp(maskCoord.x, 0, i32(maskDims.x) - 1),
              clamp(maskCoord.y, 0, i32(maskDims.y) - 1)
            );
            
            let mask = textureLoad(maskTexture, clampedMaskCoord, 0).r;
            
            // Apply blur only to background (mask < 0.5)
            // Note: mask values are typically 0 for background, 1 for foreground
            var finalColor = originalColor;
            if (mask < 0.5) {
              // Apply blur effect to background - increased kernel size to match WebGL2
              var blurredColor = vec4<f32>(0.0);
              var totalWeight = 0.0;
              
              // Use 9x9 kernel like WebGL2 (from -4 to +4)
              for (var x = -4; x <= 4; x++) {
                for (var y = -4; y <= 4; y++) {
                  let sampleCoord = coord + vec2<i32>(x, y);
                  if (sampleCoord.x >= 0 && sampleCoord.x < i32(inputDims.x) && 
                      sampleCoord.y >= 0 && sampleCoord.y < i32(inputDims.y)) {
                    // Use same weighting as WebGL2: 1.0 / (1.0 + distance)
                    let distance = length(vec2<f32>(f32(x), f32(y)));
                    let weight = 1.0 / (1.0 + distance);
                    blurredColor += textureLoad(inputTexture, sampleCoord, 0) * weight;
                    totalWeight += weight;
                  }
                }
              }
              blurredColor /= totalWeight;
              finalColor = blurredColor;
            }
            
            // Debug: visualize mask by tinting background blue for testing
            // This will help us see if the mask is working correctly
            if (mask < 0.5) {
              finalColor = mix(finalColor, vec4<f32>(0.2, 0.2, 1.0, 1.0), 0.3); // Blue tint for background
            }
            
            textureStore(outputTexture, coord, finalColor);
          }
        `;

        const computeShader = device.createShaderModule({
          code: computeShaderCode,
        });

        const computePipeline = device.createComputePipeline({
          layout: 'auto',
          compute: {
            module: computeShader,
            entryPoint: 'main',
          },
        });

        return {
          render: (videoElement, maskImageData) => {
            // Update canvas size if needed
            if (webgpuCanvas.width !== videoElement.videoWidth || webgpuCanvas.height !== videoElement.videoHeight) {
              webgpuCanvas.width = videoElement.videoWidth;
              webgpuCanvas.height = videoElement.videoHeight;
              // Reconfigure context with new size
              context.configure({
                device: device,
                format: format,
              });
            }
            
            // WebGPU rendering implementation (which currently falls back to 2D canvas)
            // Note: renderWithWebGPU now handles copying to main canvas directly
            renderWithWebGPU(device, context, computePipeline, videoElement, maskImageData, webgpuCanvas);
          }
        };
      }

      // Helper function to create WebGL program
      function createProgram(gl, vertexSource, fragmentSource) {
        const vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexSource);
        const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentSource);
        const program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);
        return program;
      }

      function createShader(gl, type, source) {
        const shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        return shader;
      }

      function renderWithWebGL2(gl, program, videoElement, maskImageData, resources) {
        const { positionBuffer, videoTexture, maskTexture, 
                positionLocation, texCoordLocation, imageLocation, 
                maskLocation, resolutionLocation, blurAmountLocation, webglCanvas } = resources;
        
        // Update canvas size if needed
        if (webglCanvas.width !== videoElement.videoWidth || webglCanvas.height !== videoElement.videoHeight) {
          webglCanvas.width = videoElement.videoWidth;
          webglCanvas.height = videoElement.videoHeight;
        }
        
        gl.viewport(0, 0, webglCanvas.width, webglCanvas.height);
        gl.useProgram(program);
        
        // Update video texture
        gl.bindTexture(gl.TEXTURE_2D, videoTexture);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
        gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, videoElement);
        
        // Update mask texture
        gl.bindTexture(gl.TEXTURE_2D, maskTexture);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
        gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, maskImageData);
        
        // Set up vertex attributes
        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        gl.enableVertexAttribArray(positionLocation);
        gl.vertexAttribPointer(positionLocation, 2, gl.FLOAT, false, 16, 0);
        gl.enableVertexAttribArray(texCoordLocation);
        gl.vertexAttribPointer(texCoordLocation, 2, gl.FLOAT, false, 16, 8);
        
        // Set uniforms
        gl.activeTexture(gl.TEXTURE0);
        gl.bindTexture(gl.TEXTURE_2D, videoTexture);
        gl.uniform1i(imageLocation, 0);
        
        gl.activeTexture(gl.TEXTURE1);
        gl.bindTexture(gl.TEXTURE_2D, maskTexture);
        gl.uniform1i(maskLocation, 1);
        
        gl.uniform2f(resolutionLocation, webglCanvas.width, webglCanvas.height);
        gl.uniform1f(blurAmountLocation, 6.0); // Blur amount
        
        // Draw
        gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
      }

      function renderWithWebGPU(device, context, computePipeline, videoElement, maskImageData, webgpuCanvas) {
        try {
          console.log('Attempting WebGPU rendering...');
          
          // Update canvas size if needed
          if (webgpuCanvas.width !== videoElement.videoWidth || webgpuCanvas.height !== videoElement.videoHeight) {
            webgpuCanvas.width = videoElement.videoWidth;
            webgpuCanvas.height = videoElement.videoHeight;
            context.configure({
              device: device,
              format: 'bgra8unorm',
            });
          }
          
          const width = webgpuCanvas.width;
          const height = webgpuCanvas.height;
          
          // Create textures for input video and mask
          const videoTexture = device.createTexture({
            size: [width, height, 1],
            format: 'rgba8unorm',
            usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
          });
          
          const maskTexture = device.createTexture({
            size: [maskImageData.width, maskImageData.height, 1],
            format: 'rgba8unorm',
            usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
          });
          
          const outputTexture = device.createTexture({
            size: [width, height, 1],
            format: 'rgba8unorm',
            usage: GPUTextureUsage.STORAGE_BINDING | GPUTextureUsage.COPY_SRC | GPUTextureUsage.TEXTURE_BINDING,
          });
          
          // Create a temporary canvas to extract video frame data
          const tempCanvas = document.createElement('canvas');
          tempCanvas.width = width;
          tempCanvas.height = height;
          const tempCtx = tempCanvas.getContext('2d');
          tempCtx.drawImage(videoElement, 0, 0, width, height);
          const videoImageData = tempCtx.getImageData(0, 0, width, height);
          
          // Upload video data to GPU
          device.queue.writeTexture(
            { texture: videoTexture },
            videoImageData.data,
            { bytesPerRow: width * 4 },
            [width, height, 1]
          );
          
          // Upload mask data to GPU
          console.log('Uploading mask data:', maskImageData.width, 'x', maskImageData.height, 'bytes:', maskImageData.data.length);
          
          // Debug: check some mask values
          const sampleMaskValues = [];
          for (let i = 0; i < Math.min(10, maskImageData.data.length / 4); i++) {
            sampleMaskValues.push(maskImageData.data[i * 4]); // R channel
          }
          console.log('Sample mask values (first 10 pixels):', sampleMaskValues);
          
          device.queue.writeTexture(
            { texture: maskTexture },
            maskImageData.data,
            { bytesPerRow: maskImageData.width * 4 },
            [maskImageData.width, maskImageData.height, 1]
          );
          
          // Create bind group
          const bindGroup = device.createBindGroup({
            layout: computePipeline.getBindGroupLayout(0),
            entries: [
              {
                binding: 0,
                resource: videoTexture.createView(),
              },
              {
                binding: 1,
                resource: maskTexture.createView(),
              },
              {
                binding: 2,
                resource: outputTexture.createView(),
              },
            ],
          });
          
          // Run compute shader
          const commandEncoder = device.createCommandEncoder();
          const computePass = commandEncoder.beginComputePass();
          computePass.setPipeline(computePipeline);
          computePass.setBindGroup(0, bindGroup);
          
          const workgroupCountX = Math.ceil(width / 8);
          const workgroupCountY = Math.ceil(height / 8);
          computePass.dispatchWorkgroups(workgroupCountX, workgroupCountY);
          computePass.end();
          
          // Create a simple render pipeline to copy rgba to bgra canvas
          const vertexShader = device.createShaderModule({
            code: `
              @vertex
              fn main(@builtin(vertex_index) vertexIndex: u32) -> @builtin(position) vec4<f32> {
                var pos = array<vec2<f32>, 6>(
                  vec2<f32>(-1.0, -1.0),
                  vec2<f32>( 1.0, -1.0),
                  vec2<f32>(-1.0,  1.0),
                  vec2<f32>( 1.0, -1.0),
                  vec2<f32>( 1.0,  1.0),
                  vec2<f32>(-1.0,  1.0)
                );
                return vec4<f32>(pos[vertexIndex], 0.0, 1.0);
              }
            `
          });
          
          const fragmentShader = device.createShaderModule({
            code: `
              @group(0) @binding(0) var inputTexture: texture_2d<f32>;
              @group(0) @binding(1) var textureSampler: sampler;
              
              @fragment
              fn main(@builtin(position) coord: vec4<f32>) -> @location(0) vec4<f32> {
                let uv = coord.xy / vec2<f32>(${width}.0, ${height}.0);
                return textureSample(inputTexture, textureSampler, uv);
              }
            `
          });
          
          const renderPipeline = device.createRenderPipeline({
            layout: 'auto',
            vertex: {
              module: vertexShader,
              entryPoint: 'main',
            },
            fragment: {
              module: fragmentShader,
              entryPoint: 'main',
              targets: [{
                format: 'bgra8unorm',
              }],
            },
            primitive: {
              topology: 'triangle-list',
            },
          });
          
          const sampler = device.createSampler({
            magFilter: 'linear',
            minFilter: 'linear',
          });
          
          const renderBindGroup = device.createBindGroup({
            layout: renderPipeline.getBindGroupLayout(0),
            entries: [
              {
                binding: 0,
                resource: outputTexture.createView(),
              },
              {
                binding: 1,
                resource: sampler,
              },
            ],
          });
          
          // Render to canvas
          const canvasTexture = context.getCurrentTexture();
          const renderPass = commandEncoder.beginRenderPass({
            colorAttachments: [{
              view: canvasTexture.createView(),
              clearValue: { r: 0, g: 0, b: 0, a: 1 },
              loadOp: 'clear',
              storeOp: 'store',
            }],
          });
          
          renderPass.setPipeline(renderPipeline);
          renderPass.setBindGroup(0, renderBindGroup);
          renderPass.draw(6);
          renderPass.end();
          
          device.queue.submit([commandEncoder.finish()]);
          
          // Copy WebGPU canvas to main canvas
          const mainCtx = canvas.getContext('2d');
          mainCtx.clearRect(0, 0, canvas.width, canvas.height);
          mainCtx.drawImage(webgpuCanvas, 0, 0);
          
          // Clean up textures
          videoTexture.destroy();
          maskTexture.destroy();
          outputTexture.destroy();
          
        } catch (error) {
          console.warn('WebGPU rendering failed, falling back to 2D canvas:', error);
          // Fallback to 2D canvas rendering
          const fallbackCanvas = document.createElement('canvas');
          fallbackCanvas.width = webgpuCanvas.width;
          fallbackCanvas.height = webgpuCanvas.height;
          
          const ctx = fallbackCanvas.getContext('2d');
          ctx.clearRect(0, 0, fallbackCanvas.width, fallbackCanvas.height);
          ctx.drawImage(videoElement, 0, 0, fallbackCanvas.width, fallbackCanvas.height);
          
          // Apply simple blur effect using 2D canvas
          const imageData = ctx.getImageData(0, 0, fallbackCanvas.width, fallbackCanvas.height);
          const blurredData = applySimpleBlur(imageData, maskImageData);
          ctx.putImageData(blurredData, 0, 0);
          
          // Copy fallback result to main canvas
          const mainCtx = canvas.getContext('2d');
          mainCtx.clearRect(0, 0, canvas.width, canvas.height);
          mainCtx.drawImage(fallbackCanvas, 0, 0);
        }
      }

      function fallbackTo2DRendering(videoElement, maskImageData) {
        console.log('Using 2D canvas fallback rendering');
        // Fallback to 2D canvas with manual blur effect
        const ctx = canvas.getContext('2d');
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Draw video
        ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
        
        // Apply simple blur effect using 2D canvas
        const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
        const blurredData = applySimpleBlur(imageData, maskImageData);
        ctx.putImageData(blurredData, 0, 0);
      }

      function applySimpleBlur(imageData, maskImageData) {
        console.log('Applying simple blur with mask data');
        
        // Validate input data
        if (!imageData || !imageData.data || imageData.width <= 0 || imageData.height <= 0) {
          console.error('Invalid imageData:', imageData);
          return imageData;
        }
        
        if (!maskImageData || !maskImageData.data || maskImageData.width <= 0 || maskImageData.height <= 0) {
          console.error('Invalid maskImageData:', maskImageData);
          return imageData;
        }
        
        // Simple blur implementation for fallback
        const data = imageData.data;
        const maskData = maskImageData.data;
        const width = imageData.width;
        const height = imageData.height;
        
        // Create a copy for output with validation
        const outputData = new ImageData(width, height);
        const output = outputData.data;
        
        // Scale factors for mask vs image dimensions
        const maskWidth = maskImageData.width;
        const maskHeight = maskImageData.height;
        const scaleX = maskWidth / width;
        const scaleY = maskHeight / height;
        
        for (let y = 0; y < height; y++) {
          for (let x = 0; x < width; x++) {
            const idx = (y * width + x) * 4;
            
            // Get corresponding mask pixel
            const maskX = Math.floor(x * scaleX);
            const maskY = Math.floor(y * scaleY);
            const maskIdx = (maskY * maskWidth + maskX) * 4;
            const maskValue = maskData[maskIdx] / 255; // Normalize mask value
            
            if (maskValue < 0.5) { // Background pixel, apply blur
              // Stronger 5x5 blur for more visible effect
              let r = 0, g = 0, b = 0, count = 0;
              for (let dy = -2; dy <= 2; dy++) {
                for (let dx = -2; dx <= 2; dx++) {
                  const nx = x + dx;
                  const ny = y + dy;
                  if (nx >= 0 && nx < width && ny >= 0 && ny < height) {
                    const nidx = (ny * width + nx) * 4;
                    r += data[nidx];
                    g += data[nidx + 1];
                    b += data[nidx + 2];
                    count++;
                  }
                }
              }
              output[idx] = r / count;
              output[idx + 1] = g / count;
              output[idx + 2] = b / count;
              output[idx + 3] = data[idx + 3];
            } else { // Foreground pixel, keep original
              output[idx] = data[idx];
              output[idx + 1] = data[idx + 1];
              output[idx + 2] = data[idx + 2];
              output[idx + 3] = data[idx + 3];
            }
          }
        }
        return outputData;
      }

      // Initialize both segmenter and renderer
      await initializeSegmenter();
      await initializeBlurRenderer();

      // Handle renderer switching
      useWebGPUCheckbox.addEventListener('change', async () => {
        status.innerText = 'Switching renderer...';
        await initializeBlurRenderer();
        // Update status to show both segmentation and renderer
        const segmentationType = useGPUSegmentationCheckbox.checked ? 'GPU (TensorFlow.js)' : 'CPU (MediaPipe)';
        const rendererType = useWebGPUCheckbox.checked ? 'WebGPU' : 'WebGL2';
        status.innerText = `Segmentation: ${segmentationType} | Renderer: ${rendererType}`;
        // Reset counters when switching renderers
        count = 0;
        segmentTimes.length = 0;
      });

      // Handle segmentation switching
      useGPUSegmentationCheckbox.addEventListener('change', async () => {
        status.innerText = 'Switching segmentation...';
        await initializeSegmenter();
        // Reset counters when switching segmentation
        count = 0;
        segmentTimes.length = 0;
      });

      const stream = await navigator.mediaDevices.getUserMedia({
        video: { frameRate: { exact: 30 }, width: 1280, height: 720 },
      });
      video.srcObject = stream;
      await new Promise((resolve) => (video.onloadedmetadata = resolve));

      // If modelType is landscape, the model resolution is 256x144.
      // If modelType is general, the model resolution is 256x256.
      // Initialize segmenter
      await initializeSegmenter();

      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;

      async function render() {
        const start = performance.now();
        const segmentation = await segmenter.segmentPeople(video);

        // Readback mask data (handling both CPU and GPU segmentation)
        let maskImageData = null;
        for (let segment of segmentation) {
          const mask = segment.mask;
          try {
            console.log('Mask object:', mask, 'Type:', typeof mask);
            
            if (useGPUSegmentationCheckbox.checked) {
              // GPU segmentation returns a tensor, need to read back to ImageData
              console.log('Converting GPU tensor mask to ImageData...');
              if (mask && typeof mask.toImageData === 'function') {
                maskImageData = await mask.toImageData();
              } else if (mask && typeof mask.dataSync === 'function') {
                // Alternative: manual conversion from tensor
                const maskData = await mask.data();
                const [height, width] = mask.shape;
                console.log('Mask dimensions from tensor:', width, 'x', height);
                
                // Create ImageData from tensor data
                const imageData = new ImageData(width, height);
                for (let i = 0; i < maskData.length; i++) {
                  const value = maskData[i] * 255; // Convert from 0-1 to 0-255
                  imageData.data[i * 4] = value;     // R
                  imageData.data[i * 4 + 1] = value; // G
                  imageData.data[i * 4 + 2] = value; // B
                  imageData.data[i * 4 + 3] = 255;   // A
                }
                maskImageData = imageData;
              }
            } else {
              // CPU segmentation already returns ImageData format
              console.log('Converting CPU mask to ImageData...');
              maskImageData = await mask.toImageData();
            }
            
            // Validate mask data
            if (!maskImageData || maskImageData.width <= 0 || maskImageData.height <= 0) {
              console.error('Invalid mask data:', maskImageData);
              maskImageData = null;
            } else {
              console.log('Valid mask data:', maskImageData.width, 'x', maskImageData.height);
            }
            
            break; // Use first segment
          } catch (error) {
            console.error('Failed to convert mask to ImageData:', error);
            maskImageData = null;
          }
        }

        const segmentTime = performance.now() - start;
        const minutes = 1;
        count++;
        if (((performance.now() - startRun) <= 1000 * 60 * minutes)) { // only record for 1 minute
          console.log(`Segmentation time ${count}: ${segmentTime.toFixed(2)} ms`);
          if (count > 3) {
            // Skip first 3 inferences, treat as warmup
            segmentTimes.push(segmentTime);
          }
        } else {
          if (status.innerText.includes('Segmentation:') && !status.innerText.includes('median time:')) {
            const medianTime = getMedianValue(segmentTimes);
            const segmentationType = useGPUSegmentationCheckbox.checked ? 'GPU (TensorFlow.js)' : 'CPU (MediaPipe)';
            const rendererType = useWebGPUCheckbox.checked ? 'WebGPU' : 'WebGL2';
            status.innerText = `Segmentation: ${segmentationType} | Renderer: ${rendererType} | Running ${minutes} minute(s), median time: ${medianTime.toFixed(2)} ms`;
          }
        }

        // Use our custom blur renderer (WebGPU or WebGL2)
        if (blurRenderer && maskImageData) {
          try {
            blurRenderer.render(video, maskImageData);
          } catch (error) {
            console.error('Blur rendering failed:', error);
            // Fallback to simple video display
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
          }
        } else {
          // Fallback to simple canvas rendering
          ctx.clearRect(0, 0, canvas.width, canvas.height);
          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        }

        if (QueryString.fps !== '24') {
          requestAnimationFrame(render);
        }
      }

      if (QueryString.fps === '24') {
        setInterval(render, 1000/vbFps);
      } else {
        render();
      }
    }

    run();
  </script>
</body>

</html>
